{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of classes: 38\n",
      "\n",
      "With ADC interpolation to length 512:\n",
      "ADC1 shape: torch.Size([512])\n",
      "ADC2 shape: torch.Size([512])\n",
      "Audio Sampling Rate: 48000\n",
      "ADC Highcut: 3700\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "from scipy import signal\n",
    "import os\n",
    "\n",
    "class MemmapDataset(Dataset):\n",
    "    def __init__(self, descriptor_path, padding_handling=\"remove\", interp_length=None, transform=None, filter=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            descriptor_path (str): Path to the descriptor JSON file (e.g., 'descriptor.json').\n",
    "            padding_handling (str or float): How to handle np.inf padding values.\n",
    "                - \"remove\" (default): Remove the padded np.inf values and return variable-length arrays.\n",
    "                - A float: Replace any np.inf values with the given float.\n",
    "            interp_length (int, optional): If provided, the ADC data (adc1 and adc2) will be\n",
    "                first stripped of np.inf padding and then interpolated to this fixed length.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "            filter (bool, optional): Whether to apply a bandpass filter to the audio data.\n",
    "        \"\"\"\n",
    "        # Load descriptor from JSON file.\n",
    "        with open(descriptor_path, 'r') as f:\n",
    "            self.descriptor = json.load(f)\n",
    "        \n",
    "        # Extract required parameters from the descriptor.\n",
    "        self.audio_sampling_rate = self.descriptor['audio_sampling_rate']\n",
    "        self.adc_sampling_rate = self.descriptor['adc_sampling_rate']\n",
    "        self.audio_lowcut       = self.descriptor['audio_lowcut']\n",
    "        self.audio_highcut      = self.descriptor['audio_highcut']\n",
    "        self.adc_lowcut         = self.descriptor['adc_lowcut']\n",
    "        self.adc_highcut        = self.descriptor['adc_highcut']\n",
    "        self.max_audio_len      = self.descriptor['max_audio_len']\n",
    "        self.max_adc_len        = self.descriptor['max_adc_len']\n",
    "        self.n_segments         = self.descriptor['n_segments']\n",
    "        self.memmap_filename    = self.descriptor['memmap_filename']\n",
    "        self.dataset_mapping    = self.descriptor['dataset_mapping']\n",
    "        # Rebuild the dtype from the descriptor.\n",
    "        self.dtype = np.dtype([tuple(item) for item in self.descriptor['dtype']])\n",
    "        \n",
    "        # Open the memmap file in read-only mode using the number of segments from the descriptor.\n",
    "        self.memmap = np.memmap(self.memmap_filename, dtype=self.dtype, mode='r', shape=(self.n_segments,))\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.padding_handling = padding_handling\n",
    "        self.interp_length = interp_length\n",
    "        self.filter = filter\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_segments\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve the record from the memmap.\n",
    "        row = self.memmap[index]\n",
    "        \n",
    "        # Convert fixed-size arrays to numpy arrays.\n",
    "        audio_arr = np.array(row['audio'])\n",
    "        adc1_arr = np.array(row['adc1'])\n",
    "        adc2_arr = np.array(row['adc2'])\n",
    "        \n",
    "        # Process audio channel using the padding handling method.\n",
    "        audio_arr = self._handle_padding(audio_arr, self.padding_handling)\n",
    "        adc1_arr = self._handle_padding(adc1_arr, self.padding_handling)\n",
    "        adc2_arr = self._handle_padding(adc2_arr, self.padding_handling)\n",
    "        \n",
    "        if self.filter:\n",
    "            audio_arr = self.BPfilter(audio_arr, self.audio_sampling_rate, self.audio_lowcut, self.audio_highcut)\n",
    "            adc1_arr = self.BPfilter(adc1_arr, self.adc_sampling_rate, self.adc_lowcut, self.adc_highcut)\n",
    "            adc2_arr = self.BPfilter(adc2_arr, self.adc_sampling_rate, self.adc_lowcut, self.adc_highcut)\n",
    "\n",
    "        # Process ADC channels.\n",
    "        if self.interp_length is not None:\n",
    "            audio_arr = self._interpolate_channel(audio_arr, self.interp_length)\n",
    "            adc1_arr = self._interpolate_channel(adc1_arr, self.interp_length)\n",
    "            adc2_arr = self._interpolate_channel(adc2_arr, self.interp_length)\n",
    "        \n",
    "        # Create a sample tuple.\n",
    "        # Use .copy() to ensure the arrays have positive strides.\n",
    "        sample = ( \n",
    "            int(row['id']), \n",
    "            torch.from_numpy(audio_arr.copy()).float(),  \n",
    "            torch.from_numpy(adc1_arr.copy()).float(), \n",
    "            torch.from_numpy(adc2_arr.copy()).float(),  \n",
    "        )\n",
    "        \n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "    def _handle_padding(self, arr, mode):\n",
    "        \"\"\"\n",
    "        Handle the np.inf padded values in the array.\n",
    "        If mode is \"remove\", return the array with inf values removed.\n",
    "        If mode is a float, replace inf values with that float.\n",
    "        \"\"\"\n",
    "        if mode == \"remove\":\n",
    "            return arr[~np.isinf(arr)]\n",
    "        elif isinstance(mode, (int, float)):\n",
    "            return np.where(np.isinf(arr), mode, arr)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid padding_handling value. Use 'remove' or a float value.\")\n",
    "\n",
    "    def _interpolate_channel(self, arr, target_length):\n",
    "        \"\"\"\n",
    "        Remove np.inf values from the array and linearly interpolate\n",
    "        to the target_length.\n",
    "        \"\"\"\n",
    "        # Remove padded inf values.\n",
    "        valid = arr[~np.isinf(arr)]\n",
    "        if len(valid) == 0:\n",
    "            # If there is no valid data, return an array of zeros.\n",
    "            return np.zeros(target_length, dtype=arr.dtype)\n",
    "        # Generate new indices for interpolation.\n",
    "        old_indices = np.arange(len(valid))\n",
    "        new_indices = np.linspace(0, len(valid) - 1, target_length)\n",
    "        return np.interp(new_indices, old_indices, valid)[:target_length]\n",
    "\n",
    "    def get(self, field):\n",
    "        \"\"\"\n",
    "        Return the value of the given descriptor field.\n",
    "        For example, dataset.get(\"audio_sampling_rate\") returns the audio sampling rate.\n",
    "        \"\"\"\n",
    "        return self.descriptor.get(field, None)\n",
    "\n",
    "    def id_to_dataset(self, id):\n",
    "        \"\"\"\n",
    "        Return the dataset string for the given ID.\n",
    "        \"\"\"\n",
    "        return self.dataset_mapping.get(str(id), \"Unknown\")\n",
    "\n",
    "    def get_Nclasses(self):\n",
    "        \"\"\"\n",
    "        Return the number of unique datasets in the dataset_mapping.\n",
    "        \"\"\"\n",
    "        return len(set(self.dataset_mapping.values()))\n",
    "    \n",
    "    def BPfilter(self, data, fs, lowcut_hz=None, highcut_hz=None):\n",
    "        \"\"\"\n",
    "        Apply a bandpass Butterworth filter to the input data.\n",
    "        \n",
    "        Parameters:\n",
    "        data : array-like\n",
    "            The input signal to filter\n",
    "        fs : float\n",
    "            Sampling frequency in Hz\n",
    "        lowcut_hz : float, optional\n",
    "            Lower cutoff frequency in Hz. If None, defaults to 20 Hz\n",
    "        highcut_hz : float, optional\n",
    "            Upper cutoff frequency in Hz. If None, defaults to fs/4 Hz\n",
    "            \n",
    "        Returns:\n",
    "        array-like\n",
    "            The filtered signal\n",
    "        \"\"\"\n",
    "        # Default cutoff frequencies if not provided.\n",
    "        if lowcut_hz is None:\n",
    "            lowcut_hz = 20  # Default lower cutoff of 20 Hz\n",
    "        if highcut_hz is None:\n",
    "            highcut_hz = fs/4  # Default upper cutoff at quarter of sampling rate\n",
    "        \n",
    "        # Convert cutoff frequencies to normalized units (0 to 1).\n",
    "        nyquist = fs / 2\n",
    "        low = lowcut_hz / nyquist\n",
    "        high = highcut_hz / nyquist\n",
    "        \n",
    "        # Create a 4th-order bandpass Butterworth filter.\n",
    "        b, a = signal.butter(2, [low, high], btype='band')\n",
    "        \n",
    "        # Apply zero-phase filtering using filtfilt.\n",
    "        filtered_data = signal.filtfilt(b, a, data)\n",
    "        return filtered_data\n",
    "\n",
    "class normalizer():\n",
    "    def __init__(self, mean, std):\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "\n",
    "    def __call__(self, sample):\n",
    "        id, audio, adc1, adc2 = sample\n",
    "        audio = (audio - self.mean[0]) / self.std[0]\n",
    "        adc1 = (adc1 - self.mean[1]) / self.std[1]\n",
    "        adc2 = (adc2 - self.mean[1]) / self.std[1]\n",
    "        return id, audio, adc1, adc2\n",
    "\n",
    "input_length = 512\n",
    "# Path to the descriptor JSON file.\n",
    "descriptor_path = 'samdescriptor.json'\n",
    "# Create a dataset instance that interpolates ADC channels to length 300, with filtering.\n",
    "# dataset = MemmapDataset(descriptor_path, padding_handling=\"remove\", interp_length=input_length, filter=True)\n",
    "dataset = MemmapDataset(descriptor_path, padding_handling=\"remove\", filter=True, interp_length=input_length)\n",
    "# dataset = MemmapDataset(descriptor_path, padding_handling=\"remove\", filter=True)\n",
    "transform = normalizer(mean=[dataset.get(\"audio_mean\"), dataset.get(\"adc_mean\")], std=[dataset.get(\"audio_std\"), dataset.get(\"adc_std\")])\n",
    "dataset.transform = transform\n",
    "output_length = dataset.get_Nclasses()\n",
    "print(\"Number of classes:\", output_length)\n",
    "sample = dataset[1]\n",
    "print(f\"\\nWith ADC interpolation to length {input_length}:\")\n",
    "print(\"ADC1 shape:\", sample[2].shape)\n",
    "print(\"ADC2 shape:\", sample[3].shape)\n",
    "\n",
    "# Demonstrate accessing a descriptor field.\n",
    "print(\"Audio Sampling Rate:\", dataset.get(\"audio_sampling_rate\"))\n",
    "print(\"ADC Highcut:\", dataset.get(\"adc_highcut\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sktime in /home/nico/.venv/lib/python3.12/site-packages (0.36.0)\n",
      "Requirement already satisfied: scikit-learn in /home/nico/.venv/lib/python3.12/site-packages (1.6.1)\n",
      "Requirement already satisfied: joblib<1.5,>=1.2.0 in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (1.4.2)\n",
      "Requirement already satisfied: numpy<2.3,>=1.21 in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (2.1.3)\n",
      "Requirement already satisfied: packaging in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (24.2)\n",
      "Requirement already satisfied: pandas<2.3.0,>=1.1 in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (2.2.3)\n",
      "Requirement already satisfied: scikit-base<0.13.0,>=0.6.1 in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (0.12.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.2 in /home/nico/.venv/lib/python3.12/site-packages (from sktime) (1.15.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /home/nico/.venv/lib/python3.12/site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/nico/.venv/lib/python3.12/site-packages (from pandas<2.3.0,>=1.1->sktime) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/nico/.venv/lib/python3.12/site-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /home/nico/.venv/lib/python3.12/site-packages (from pandas<2.3.0,>=1.1->sktime) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/nico/.venv/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<2.3.0,>=1.1->sktime) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sktime scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    segment_id, audio_arr, adc1_arr, adc2_arr = dataset[i]\n",
    "    \n",
    "    # Convert label string -> numeric\n",
    "    label_int = segment_id\n",
    "    \n",
    "    # Turn each sampleâ€™s ADC signals into (n_channels, n_timepoints).\n",
    "    # If you want a 2-channel series: \n",
    "    # shape would be (2, length)\n",
    "    # NB: if your dataset returns torch tensors, convert them to numpy\n",
    "    # For example, if they are torch tensors, do .numpy() first.\n",
    "    \n",
    "    \n",
    "    # Stack them as channels\n",
    "    # shape -> (2, input_length) if input_length=512\n",
    "    sample_2ch = np.stack([adc1_arr, adc2_arr], axis=0)\n",
    "    \n",
    "    X.append(sample_2ch)\n",
    "    y.append(label_int)\n",
    "\n",
    "# Convert python lists to arrays\n",
    "X = np.array(X)  # shape: (n_samples, 2, 512)\n",
    "y = np.array(y)  # shape: (n_samples,)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42, \n",
    "                                                    stratify=y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rocket has been created.\n",
      "Rocket has been fitted.\n",
      "Data has been transformed.\n",
      "Classifier has been fitted.\n",
      "Test accuracy: 0.77728285077951\n"
     ]
    }
   ],
   "source": [
    "from sktime.transformations.panel.rocket import Rocket\n",
    "from sklearn.linear_model import RidgeClassifierCV\n",
    "\n",
    "# 1) Create Rocket transformer\n",
    "rocket = Rocket(num_kernels=1000, random_state=42)\n",
    "print(\"Rocket has been created.\")\n",
    "# 2) Fit Rocket on the training set\n",
    "rocket.fit(X_train)\n",
    "print(\"Rocket has been fitted.\")\n",
    "# 3) Transform train and test\n",
    "X_train_transformed = rocket.transform(X_train)\n",
    "X_test_transformed  = rocket.transform(X_test)\n",
    "print(\"Data has been transformed.\")\n",
    "# X_train_transformed will be shape [n_samples, 2 * num_kernels]\n",
    "# because Rocket yields two features per kernel \n",
    "# (global max pooling + proportion of positive values)\n",
    "\n",
    "# put graph here\n",
    "\n",
    "# 4) Fit a classifier\n",
    "clf = RidgeClassifierCV(alphas=np.logspace(-3,3,7))\n",
    "clf.fit(X_train_transformed, y_train)\n",
    "print(\"Classifier has been fitted.\")\n",
    "# 5) Evaluate on test\n",
    "y_pred = clf.predict(X_test_transformed)\n",
    "from sklearn.metrics import accuracy_score\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "print(\"Test accuracy:\", acc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rocket params {'n_jobs': 1, 'normalise': True, 'num_kernels': 100000, 'random_state': 42}\n",
      "rocket params {}\n",
      "clf params {'alphas': array([1.e-03, 1.e-02, 1.e-01, 1.e+00, 1.e+01, 1.e+02, 1.e+03]), 'class_weight': None, 'cv': None, 'fit_intercept': True, 'scoring': None, 'store_cv_results': None, 'store_cv_values': 'deprecated'}\n"
     ]
    }
   ],
   "source": [
    "# print(\"roclets kernels\", rocket.kernels_)\n",
    "print(\"rocket params\", rocket.get_params())\n",
    "print(\"rocket params\", rocket.get_fitted_params())\n",
    "print(\"clf params\", clf.get_params())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Function to evaluate the model and generate the confusion matrix\n",
    "def evaluate_model(model, dataloader, device='cpu'):\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for ids, audio, adc1, adc2 in tqdm(dataloader):\n",
    "            # adc1 = adc1.to(device)\n",
    "            # adc2 = adc2.to(device)\n",
    "            audio = audio.to(device)\n",
    "            audio = audio.unsqueeze(1)\n",
    "            ids = ids.to(device)\n",
    "            \n",
    "            # adc = torch.stack((adc1, adc2), dim=1)\n",
    "            outputs = model(audio)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            \n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(ids.cpu().numpy())\n",
    "    \n",
    "    return all_labels, all_preds\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "labels, preds = evaluate_model(model, val_loader, device=global_device)\n",
    "\n",
    "# Generate the confusion matrix\n",
    "conf_matrix = confusion_matrix(labels, preds)\n",
    "\n",
    "# Replace IDs with dataset names\n",
    "label_names = [dataset.id_to_dataset(label) for label in range(output_length)]\n",
    "pred_names = [dataset.id_to_dataset(pred) for pred in range(output_length)]\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=pred_names, yticklabels=label_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
